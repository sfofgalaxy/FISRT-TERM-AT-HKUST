{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction',\n",
       "  'authors': ['Samuel Broscheit',\n",
       "   'Kiril Gashteovski',\n",
       "   'Yanjie Wang',\n",
       "   'Rainer Gemulla'],\n",
       "  'abstract': 'Open Information Extraction systems extract (“subject text”, “relation text”, “object text”) triples from raw text. Some triples are textual versions of facts, i.e., non-canonicalized mentions of entities and relations. In this paper, we investigate whether it is possible to infer new facts directly from the open knowledge graph without any canonicalization or any supervision from curated knowledge. For this purpose, we propose the open link prediction task,i.e., predicting test facts by completing (“subject text”, “relation text”, ?) questions. An evaluation in such a setup raises the question if a correct prediction is actually a new fact that was induced by reasoning over the open knowledge graph or if it can be trivially explained. For example, facts can appear in different paraphrased textual variants, which can lead to test leakage. To this end, we propose an evaluation protocol and a methodology for creating the open link prediction benchmark OlpBench. We performed experiments with a prototypical knowledge graph embedding model for openlink prediction. While the task is very challenging, our results suggests that it is possible to predict genuinely new facts, which can not be trivially explained.',\n",
       "  'year': '2020'},\n",
       " {'title': 'Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings',\n",
       "  'authors': ['Apoorv Saxena', 'Aditay Tripathi', 'Partha Talukdar'],\n",
       "  'abstract': 'Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG. Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer. KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA. Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text, which isn’t always readily available. In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction. Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far. We fill this gap in this paper and propose EmbedKGQA. EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs. EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods. Through extensive experiments on multiple benchmark datasets, we demonstrate EmbedKGQA’s effectiveness over other state-of-the-art baselines.',\n",
       "  'year': '2020'},\n",
       " {'title': 'A Relational Memory-based Embedding Model for Triple Classification and Search Personalization',\n",
       "  'authors': ['Dai Quoc Nguyen', 'Tu Nguyen', 'Dinh Phung'],\n",
       "  'abstract': 'Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems. To this end, we introduce a novel embedding model, named R-MeN, that explores a relational memory network to encode potential dependencies in relationship triples. R-MeN considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism. Thus R-MeN encodes new information from interactions between the memory and each input vector to return a corresponding vector. Consequently, R-MeN feeds these 3 returned vectors to a convolutional neural network-based decoder to produce a scalar score for the triple. Experimental results show that our proposed R-MeN obtains state-of-the-art results on SEARCH17 for the search personalization task, and on WN11 and FB13 for the triple classification task.',\n",
       "  'year': '2020'},\n",
       " {'title': 'Low-Dimensional Hyperbolic Knowledge Graph Embeddings',\n",
       "  'authors': ['Ines Chami',\n",
       "   'Adva Wolf',\n",
       "   'Da-Cheng Juan',\n",
       "   'Frederic Sala',\n",
       "   'Sujith Ravi',\n",
       "   'Christopher Ré'],\n",
       "  'abstract': 'Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns. Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that different geometric transformations capture different types of relations while attention- based transformations generalize to multiple relations. In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10.',\n",
       "  'year': '2020'},\n",
       " {'title': 'Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation',\n",
       "  'authors': ['Chao Zhao', 'Marilyn Walker', 'Snigdha Chaturvedi'],\n",
       "  'abstract': 'Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.',\n",
       "  'year': '2020'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unittest import result\n",
    "import nltk as n\n",
    "import json\n",
    "import math\n",
    "import collections\n",
    "\n",
    "'''\n",
    "retrieve all abstarcts of papers as a list\n",
    "return papers and list of abstracts without punctuation marks\n",
    "'''\n",
    "def get_abstract_list():\n",
    "    # get all data from \"paper.json\"\n",
    "    with open('./paper.json','r') as f:\n",
    "        papers = json.load(f)\n",
    "    \n",
    "    # define the stop_words, original abstract list and initailize filted abstracts\n",
    "    abstract_list = [item['abstract'] for item in papers]\n",
    "    filtered_abstracts = []\n",
    "\n",
    "    # filter every abstract in abstract list\n",
    "    for abstract in abstract_list:\n",
    "        # perform tokenization\n",
    "        word_tokens = tokenizer.tokenize(abstract)\n",
    "        # filter the stop word\n",
    "        filtered_sentence = [w.lower() for w in word_tokens]\n",
    "        filtered_abstracts.append(filtered_sentence) \n",
    "    \n",
    "    return papers, filtered_abstracts\n",
    "\n",
    "'''\n",
    "stem all abstarcts in the list and create inverted index for it if it is not stop word\n",
    "return inverted index before removing stop words\n",
    "'''\n",
    "def stem_and_index(abstracts_list):\n",
    "    # initialize the stemmer and stemmed list\n",
    "    invert_index = dict()\n",
    "    # stem every abstract and create inverted index\n",
    "    for i, abstract in enumerate(abstracts_list):\n",
    "        # create inverted index\n",
    "        for j, word in enumerate(abstract):\n",
    "            stem_word = ps.stem(word)\n",
    "            # if it is stop word, don't create index\n",
    "            if stem_word not in stop_words:\n",
    "                dic = dict()\n",
    "                # add it to inverted index (word -> document which contains it)\n",
    "                if stem_word in invert_index:\n",
    "                    dic = invert_index[stem_word]\n",
    "                else:\n",
    "                    invert_index[stem_word] = dic\n",
    "                # if word is the first time in this document, create new list\n",
    "                if i not in dic:\n",
    "                    dic[i] = []\n",
    "                dic[i].append(j)\n",
    "    \n",
    "    return invert_index\n",
    "\n",
    "''' \n",
    "return number of the phase contained by the document\n",
    "document_id is a the number of document\n",
    "'''\n",
    "def count_phase_number(document_id, word_tokens):\n",
    "    word = word_tokens[0]\n",
    "    count = 0\n",
    "    if word in invert_index and document_id in invert_index[word]:\n",
    "        for index in invert_index[word][document_id]:\n",
    "            i = index\n",
    "            flag = True\n",
    "            for w in word_tokens:\n",
    "                if i >= len(abstracts_list[document_id]) or not w == ps.stem(abstracts_list[document_id][i]):\n",
    "                    flag = False\n",
    "                    break\n",
    "                i += 1\n",
    "        count += flag\n",
    "    return count\n",
    "\n",
    "'''\n",
    "compute the cosine similarity between document weight and query weight\n",
    "return a similarity number like 10\n",
    "'''\n",
    "def compute_weight_and_similarity(document_id, word_weight, phase_weight):\n",
    "    # initialize document weight and number of document N\n",
    "    document_weight = []\n",
    "    N = len(abstracts_list)\n",
    "    # comput norm of the query_weight\n",
    "    query_norm = 0\n",
    "    query_weight = collections.OrderedDict()\n",
    "    query_weight.update(word_weight)\n",
    "    query_weight.update(phase_weight)\n",
    "    for weight in query_weight.values():\n",
    "        query_norm += (weight ** 2)\n",
    "    query_norm = math.sqrt(query_norm)\n",
    "\n",
    "    # compute the one document weight of the word  \n",
    "    for word in word_weight:\n",
    "        weight = 0\n",
    "        if word in invert_index:\n",
    "            document_index = invert_index[word]\n",
    "            if document_id in document_index:\n",
    "                tf = len(document_index[document_id])\n",
    "                idf =  math.log2(1.0*N/len(document_index))\n",
    "                weight = tf * idf\n",
    "            # add weight to the document weight list\n",
    "        document_weight.append(weight)\n",
    "\n",
    "    # compute the one document weight of the phase  \n",
    "    for phase in phase_weight:\n",
    "        word_tokens = tokenizer.tokenize(phase)\n",
    "        word_tokens = [ps.stem(word) for word in word_tokens]\n",
    "        weight = 0\n",
    "        if word_tokens[0] in invert_index:\n",
    "            document_index = invert_index[word_tokens[0]]\n",
    "            if document_id in document_index:\n",
    "                tf = count_phase_number(document_id, word_tokens)\n",
    "                if tf:\n",
    "                    document_count = 0\n",
    "                    for id in document_index:\n",
    "                        if count_phase_number(id, word_tokens):\n",
    "                            document_count += 1\n",
    "                    idf =  math.log2(1.0*N/document_count)\n",
    "                    weight = tf * idf\n",
    "                # add weight to the document weight list\n",
    "        document_weight.append(weight)\n",
    "\n",
    "    # compute similarity between document and query\n",
    "    cosine_similarity = 0\n",
    "    document_norm = 0\n",
    "    for i, weight in enumerate(query_weight.values()):\n",
    "        cosine_similarity += document_weight[i] * weight\n",
    "        document_norm += (document_weight[i] ** 2)\n",
    "    document_norm = math.sqrt(document_norm)\n",
    "\n",
    "    # compute the cosine similarity\n",
    "    if document_norm == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return cosine_similarity / document_norm / query_norm\n",
    "\n",
    "'''\n",
    "handle the query, return top 5 id\n",
    "1. stem the query and get query weight\n",
    "2. compute document weight of the query\n",
    "3. compute the similairity and sort\n",
    "'''\n",
    "def search(word_weight, phase_weight):\n",
    "    # find documents which contain any of query item\n",
    "    # compute similarity score of the article (similarity of Q and each D) and sort\n",
    "    similarity = dict()\n",
    "    for word in word_weight:\n",
    "        # if word don't occur, document similarity don't need to be computed\n",
    "        if word in invert_index:\n",
    "            for id in invert_index[word]:\n",
    "                # compute the similarity bettween document and query weight if haven't computed\n",
    "                if id not in similarity:\n",
    "                    similarity[id] = compute_weight_and_similarity(id, word_weight, phase_weight)\n",
    "    \n",
    "    for phase in phase_weight:\n",
    "        word_tokens = tokenizer.tokenize(phase)\n",
    "        word_tokens = [ps.stem(word) for word in word_tokens]\n",
    "        # for phase, we only need to concentrate on the first word. if the first word has been invert index\n",
    "        if word_tokens[0] in invert_index:\n",
    "            for id in invert_index[word_tokens[0]]:\n",
    "                # compute the similarity bettween document and query weight if haven't computed\n",
    "                if id not in similarity:\n",
    "                    similarity[id] = compute_weight_and_similarity(id, word_weight, phase_weight)\n",
    "    \n",
    "    sored_items = sorted(similarity.items(), key = lambda item:item[1], reverse=True)\n",
    "    top5 = []\n",
    "    for i in range(5):\n",
    "        top5.append(sored_items[i][0])\n",
    "    return top5\n",
    "\n",
    "'''\n",
    "input a query like 'a apple \"english teacher\"'\n",
    "output a list of words and phases with stopwords removal\n",
    "'''\n",
    "def query_to_list(query):\n",
    "    # split the query into list\n",
    "    # for mark ' \" ' Left double quotation mark\n",
    "    flag = False\n",
    "    # record phase (Wrapped in double quotes)\n",
    "    s = \"\"\n",
    "    # other character\n",
    "    other = \"\"\n",
    "    phase_tokens = []\n",
    "    for ch in query:\n",
    "        if ch == '\"':\n",
    "            if flag:\n",
    "                phase_tokens.append(s)\n",
    "                s = \"\"\n",
    "            flag = not flag\n",
    "        elif flag:\n",
    "            s +=ch\n",
    "        else:\n",
    "            other += ch\n",
    "    # merge other words and phase into a list, filter stopwords\n",
    "    word_tokens = tokenizer.tokenize(other)\n",
    "    return [w for w in word_tokens if not w in stop_words], phase_tokens\n",
    "\n",
    "'''\n",
    "input the filtered word, phase (tokenized, lowered and punctuation removed)\n",
    "return word weight and phase weight\n",
    "'''\n",
    "def create_query_weight(filered_word, filtered_phase):\n",
    "    # create query weight dictionary\n",
    "    word_weight = collections.OrderedDict()\n",
    "    for word in filered_word:\n",
    "        stem_word = ps.stem(word)\n",
    "        if stem_word in word_weight:\n",
    "            word_weight[stem_word] += 1\n",
    "        else:\n",
    "            word_weight[stem_word] = 1\n",
    "\n",
    "    phase_weight = collections.OrderedDict()\n",
    "    for phase in filtered_phase:\n",
    "        if phase in phase_weight:\n",
    "            phase_weight[phase] += 1\n",
    "        else:\n",
    "            phase_weight[phase] = 1\n",
    "    \n",
    "    return word_weight, phase_weight\n",
    "    \n",
    "\n",
    "def search_api(query):\n",
    "    \"\"\"\n",
    "    query:[string] \n",
    "    return: list of dict, each dict is a paper record of the original dataset\n",
    "    \"\"\"\n",
    "    # get query word or phase list\n",
    "    filered_word, filtered_phase = query_to_list(query)\n",
    "    # create word and phase weight\n",
    "    word_weight, phase_weight = create_query_weight(filered_word, filtered_phase)   \n",
    "    # get most cosine-similar 5 document by query weight\n",
    "    result_ids = search(word_weight, phase_weight)\n",
    "    return [papers[i] for i in result_ids]\n",
    "\n",
    "# initialize the dictionary papers and all word_set\n",
    "ps = n.stem.PorterStemmer()\n",
    "stop_words = set(n.corpus.stopwords.words('english'))\n",
    "# remove the punctuation\n",
    "tokenizer = n.tokenize.RegexpTokenizer(r'\\w+')\n",
    "# get all abstracts after punctuation removal\n",
    "papers, abstracts_list = get_abstract_list()\n",
    "# get abstracts after stemming and inverted index\n",
    "invert_index = stem_and_index(abstracts_list)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89b6bbc86ed2cb068c08d53f474823e9e5a210155e0c2ae4c7f926ec6b0bd819"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
